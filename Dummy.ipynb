{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "931b9399-b981-404f-8951-3b0d273cee69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\night shift\\appdata\\local\\temp\\pip-req-build-y9sri56a\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ftfy (from clip==1.0)\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: packaging in e:\\anaconda\\envs\\assignment\\lib\\site-packages (from clip==1.0) (24.2)\n",
      "Collecting regex (from clip==1.0)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda\\envs\\assignment\\lib\\site-packages (from clip==1.0) (4.67.1)\n",
      "Collecting torch (from clip==1.0)\n",
      "  Downloading torch-2.6.0-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchvision (from clip==1.0)\n",
      "  Downloading torchvision-0.21.0-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: wcwidth in e:\\anaconda\\envs\\assignment\\lib\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Collecting filelock (from torch->clip==1.0)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\night shift\\appdata\\roaming\\python\\python313\\site-packages (from torch->clip==1.0) (4.13.0)\n",
      "Collecting networkx (from torch->clip==1.0)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch->clip==1.0)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch->clip==1.0)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in e:\\anaconda\\envs\\assignment\\lib\\site-packages (from torch->clip==1.0) (75.8.0)\n",
      "Collecting sympy==1.13.1 (from torch->clip==1.0)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch->clip==1.0)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy in e:\\anaconda\\envs\\assignment\\lib\\site-packages (from torchvision->clip==1.0) (2.2.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\anaconda\\envs\\assignment\\lib\\site-packages (from torchvision->clip==1.0) (11.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\night shift\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch->clip==1.0)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl.metadata (4.1 kB)\n",
      "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Downloading torch-2.6.0-cp313-cp313-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/204.1 MB 11.7 MB/s eta 0:00:18\n",
      "    --------------------------------------- 5.0/204.1 MB 11.7 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 7.3/204.1 MB 11.7 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 9.7/204.1 MB 11.7 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 12.1/204.1 MB 11.7 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 14.7/204.1 MB 11.7 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 17.0/204.1 MB 11.7 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 19.4/204.1 MB 11.7 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 21.0/204.1 MB 11.1 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 23.3/204.1 MB 11.2 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 25.7/204.1 MB 11.2 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 28.0/204.1 MB 11.2 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 30.1/204.1 MB 11.2 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 32.0/204.1 MB 11.1 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 34.3/204.1 MB 11.0 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 36.7/204.1 MB 11.0 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 38.8/204.1 MB 11.0 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 41.4/204.1 MB 11.0 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 43.8/204.1 MB 11.0 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 46.1/204.1 MB 11.1 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 48.5/204.1 MB 11.1 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 50.9/204.1 MB 11.1 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 53.5/204.1 MB 11.2 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 55.8/204.1 MB 11.2 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 58.2/204.1 MB 11.2 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 60.6/204.1 MB 11.2 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 62.9/204.1 MB 11.2 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 65.3/204.1 MB 11.2 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 67.9/204.1 MB 11.2 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 70.3/204.1 MB 11.3 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 72.6/204.1 MB 11.3 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 75.2/204.1 MB 11.3 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 77.6/204.1 MB 11.3 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 80.0/204.1 MB 11.3 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.6/204.1 MB 11.3 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 84.9/204.1 MB 11.3 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 87.6/204.1 MB 11.3 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 89.9/204.1 MB 11.3 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 92.3/204.1 MB 11.3 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 93.8/204.1 MB 11.2 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 95.9/204.1 MB 11.2 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 98.3/204.1 MB 11.2 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 100.7/204.1 MB 11.2 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 103.0/204.1 MB 11.2 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 105.6/204.1 MB 11.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 108.3/204.1 MB 11.3 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 110.6/204.1 MB 11.3 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 113.0/204.1 MB 11.3 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 115.6/204.1 MB 11.3 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 118.0/204.1 MB 11.3 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 120.3/204.1 MB 11.3 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 122.7/204.1 MB 11.3 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 124.8/204.1 MB 11.3 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 126.9/204.1 MB 11.2 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 129.2/204.1 MB 11.2 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 131.3/204.1 MB 11.2 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 132.1/204.1 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 133.4/204.1 MB 11.0 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 136.1/204.1 MB 11.0 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 137.9/204.1 MB 11.0 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 139.7/204.1 MB 10.9 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 141.6/204.1 MB 10.9 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 144.2/204.1 MB 10.9 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 146.5/204.1 MB 10.9 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 148.6/204.1 MB 10.9 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 150.5/204.1 MB 10.9 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 152.8/204.1 MB 10.9 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 154.9/204.1 MB 10.8 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 157.0/204.1 MB 10.8 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 159.4/204.1 MB 10.8 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 161.5/204.1 MB 10.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 163.6/204.1 MB 10.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 165.2/204.1 MB 10.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 167.2/204.1 MB 10.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 169.6/204.1 MB 10.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 172.0/204.1 MB 10.7 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 174.6/204.1 MB 10.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 177.2/204.1 MB 10.8 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 179.6/204.1 MB 10.8 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 181.9/204.1 MB 10.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 184.3/204.1 MB 10.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 186.9/204.1 MB 10.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 189.3/204.1 MB 10.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 191.6/204.1 MB 10.8 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 194.2/204.1 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 196.6/204.1 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 197.7/204.1 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  200.0/204.1 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  201.9/204.1 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.1/204.1 MB 10.3 MB/s eta 0:00:00\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading torchvision-0.21.0-cp313-cp313-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 8.0 MB/s eta 0:00:00\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 10.8 MB/s eta 0:00:00\n",
      "Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl (15 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 9.1 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py): started\n",
      "  Building wheel for clip (setup.py): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369594 sha256=764bacd85040618423a78fb6b27cbca108a5b546e30547ec4ff2916e16e9c71f\n",
      "  Stored in directory: C:\\Users\\Night Shift\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-lkrrp9sm\\wheels\\cb\\a8\\74\\5f32d6cf0407457f0f62737b6da5c14eb86b9cac476fdf630d\n",
      "Successfully built clip\n",
      "Installing collected packages: mpmath, sympy, regex, networkx, MarkupSafe, ftfy, fsspec, filelock, jinja2, torch, torchvision, clip\n",
      "Successfully installed MarkupSafe-3.0.2 clip-1.0 filelock-3.18.0 fsspec-2025.3.2 ftfy-6.3.1 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 regex-2024.11.6 sympy-1.13.1 torch-2.6.0 torchvision-0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\Night Shift\\AppData\\Local\\Temp\\pip-req-build-y9sri56a'\n"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "58ae912b-b256-47e2-be3b-8509e5244f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "170d8781-dfce-4bc6-a90b-bfc55b2fc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_clip_model(labels):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {label}\") for label in labels]).to(device)\n",
    "    return model, preprocess, text_inputs, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "180ed6a8-0fd8-4811-98b8-48658a083cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_obj_from_mask(img_path, mask_path, output_size=(244,244)):\n",
    "    image = cv2.imread(img_path)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if image is None or mask is None:\n",
    "        raise ValueError(f\"Mask: {mask_path} or image: {img_path} not found\")\n",
    "    \n",
    "    _, thresh = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    contour, _ = cv2.findContours(thresh, mode = cv2.RETR_EXTERNAL, method = cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not contour:\n",
    "        raise ValueError(f\"Not Contour was found\")\n",
    "\n",
    "    x,y,w,h = cv2.boundingRect(np.concatenate(contour))\n",
    "\n",
    "    cropped = image[y:y+h, x:x+w]\n",
    "    cropped_resized = cv2.resize(cropped, output_size)\n",
    "\n",
    "    cropped_pil = Image.fromarray(cv2.cvtColor(cropped_resized, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    return cropped_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "274d467c-5e9d-47f2-bf5e-a5a156638329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_clip(pil_img, labels = labels, text_inputs = text_inputs):\n",
    "    img_input = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_features = model.encode_image(img_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "        img_features /= img_features.norm(dim=1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        similarity = (100.0 * img_features @ text_features.T).softmax(dim=-1)\n",
    "        top_prob, top_label_idx = similarity[0].max(0)\n",
    "    \n",
    "    predicted_label = labels[top_label_idx]\n",
    "    confidence = top_prob.item()\n",
    "\n",
    "    return predicted_label, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "153d1866-5860-45f9-851b-3947e8076940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_center_crop(frame, size_ratio=0.5):\n",
    "    h, w = frame.shape[:2]\n",
    "    ch, cw = int(h * size_ratio), int(w * size_ratio)\n",
    "    if ch == 0 or cw == 0:\n",
    "        raise ValueError(\"Frame too small for cropping\")\n",
    "    x1, y1 = (w - cw) // 2, (h - ch) // 2\n",
    "    return frame[y1:y1 + ch, x1:x1 + cw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "fea965de-8d07-4086-8c57-c6d716d0c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_detection(image_root, mask_root):\n",
    "    all_detections = []\n",
    "\n",
    "    for class_folder in os.listdir(image_root):\n",
    "        image_folder = os.path.join(image_root, class_folder)\n",
    "        mask_folder = os.path.join(mask_root, class_folder)\n",
    "\n",
    "        if not os.path.isdir(image_folder):\n",
    "            continue\n",
    "\n",
    "        print(f\"🔍 Scanning folder: {class_folder}\")\n",
    "        for img_file in tqdm(os.listdir(image_folder)):\n",
    "            if not img_file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "                continue\n",
    "\n",
    "            image_path = os.path.join(image_folder, img_file)\n",
    "            mask_path = os.path.join(mask_folder, os.path.splitext(img_file)[0] + \"_mask.png\")\n",
    "\n",
    "            frame = cv2.imread(image_path)\n",
    "            if frame is None:\n",
    "                print(f\"⚠️ Skipping {img_file} — image could not be loaded.\")\n",
    "                continue\n",
    "            cropped = extract_center_crop(frame)\n",
    "\n",
    "            try:\n",
    "                cropped = extract_obj_from_mask(image_path, mask_path)\n",
    "                label, prob = classify_with_clip(cropped)\n",
    "\n",
    "                print(f\"📣 ALERT: {label.upper()} detected with {prob:.1%} confidence — {img_file}\")\n",
    "                all_detections.append((img_file, label, prob))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {img_file}: {e}\")\n",
    "\n",
    "    return all_detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "5cb33cb8-c386-4d45-b037-846bb7395671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_video_detection(video_source=0):\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    print(\"🎥 Starting video stream...\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Extract object (center crop)\n",
    "        crop = extract_center_crop(frame)\n",
    "        crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Classify\n",
    "        label, prob = classify_with_clip(crop_pil)\n",
    "        # Display alert on frame\n",
    "        # if prob > .79:\n",
    "        alert_text = f\"{label.upper()} ({prob*100:.1f}%)\"\n",
    "        cv2.putText(frame, alert_text, (30, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imshow(\"Live Detection\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"👋 Exiting...\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "f8119a35-8b0c-47bc-94b4-729672892d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"human\", \"butterfly\", \"cat\", \"dog\", \"horse\", \"elephant\", \"squirrel\"]\n",
    "model, preprocess, text_inputs, device = setup_clip_model(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "f1db8dc6-7aba-42de-ad91-e12791bd7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"animals10/raw-img/gatto/1001.jpeg\"\n",
    "mask_path = \"animals10/renamed_masks/dog/dog_0091_mask.png\"\n",
    "\n",
    "cropped_object = extract_obj_from_mask(img_path, mask_path)\n",
    "cropped_object.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "bad35d28-e70d-428f-a232-9726c904fa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔔 Detected: human (confidence: 33.30%)\n"
     ]
    }
   ],
   "source": [
    "# Classify with CLIP\n",
    "label, prob = classify_with_clip(cropped_object)\n",
    "print(f\"🔔 Detected: {label} (confidence: {prob:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "21637d6e-6cea-4ec6-8f32-6b17780784e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 Starting video stream...\n",
      "👋 Exiting...\n"
     ]
    }
   ],
   "source": [
    "run_video_detection('Untitled design.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "de618e96-f221-4be1-beb5-6a427335b073",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scanning folder: all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                           | 7/75 [00:00<00:01, 66.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing butterfly_0091.jpg: Mask: test_mask/all\\butterfly_0091_mask.png or image: Test_img/all\\butterfly_0091.jpg not found\n",
      "❌ Error processing butterfly_0092.jpg: Mask: test_mask/all\\butterfly_0092_mask.png or image: Test_img/all\\butterfly_0092.jpg not found\n",
      "❌ Error processing butterfly_0093.jpg: Mask: test_mask/all\\butterfly_0093_mask.png or image: Test_img/all\\butterfly_0093.jpg not found\n",
      "❌ Error processing butterfly_0094.jpg: Mask: test_mask/all\\butterfly_0094_mask.png or image: Test_img/all\\butterfly_0094.jpg not found\n",
      "❌ Error processing butterfly_0095.jpg: Mask: test_mask/all\\butterfly_0095_mask.png or image: Test_img/all\\butterfly_0095.jpg not found\n",
      "❌ Error processing butterfly_0096.jpg: Mask: test_mask/all\\butterfly_0096_mask.png or image: Test_img/all\\butterfly_0096.jpg not found\n",
      "❌ Error processing butterfly_0097.jpg: Mask: test_mask/all\\butterfly_0097_mask.png or image: Test_img/all\\butterfly_0097.jpg not found\n",
      "❌ Error processing butterfly_0098.jpg: Mask: test_mask/all\\butterfly_0098_mask.png or image: Test_img/all\\butterfly_0098.jpg not found\n",
      "❌ Error processing butterfly_0099.jpg: Mask: test_mask/all\\butterfly_0099_mask.png or image: Test_img/all\\butterfly_0099.jpg not found\n",
      "❌ Error processing butterfly_0100.jpg: Mask: test_mask/all\\butterfly_0100_mask.png or image: Test_img/all\\butterfly_0100.jpg not found\n",
      "❌ Error processing butterfly_0101.jpg: Mask: test_mask/all\\butterfly_0101_mask.png or image: Test_img/all\\butterfly_0101.jpg not found\n",
      "❌ Error processing butterfly_0102.jpg: Mask: test_mask/all\\butterfly_0102_mask.png or image: Test_img/all\\butterfly_0102.jpg not found\n",
      "❌ Error processing butterfly_0103.jpg: Mask: test_mask/all\\butterfly_0103_mask.png or image: Test_img/all\\butterfly_0103.jpg not found\n",
      "❌ Error processing butterfly_0104.jpg: Mask: test_mask/all\\butterfly_0104_mask.png or image: Test_img/all\\butterfly_0104.jpg not found\n",
      "❌ Error processing butterfly_0105.jpg: Mask: test_mask/all\\butterfly_0105_mask.png or image: Test_img/all\\butterfly_0105.jpg not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|█████████████████▍                                                                | 16/75 [00:00<00:02, 22.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📣 ALERT: DOG detected with 97.9% confidence — dog_4853.jpeg\n",
      "📣 ALERT: DOG detected with 98.5% confidence — dog_4854.jpeg\n",
      "📣 ALERT: DOG detected with 83.8% confidence — dog_4855.jpeg\n",
      "📣 ALERT: DOG detected with 98.3% confidence — dog_4856.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████▊                                                            | 20/75 [00:02<00:08,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📣 ALERT: DOG detected with 97.9% confidence — dog_4857.jpeg\n",
      "📣 ALERT: DOG detected with 98.9% confidence — dog_4858.jpeg\n",
      "📣 ALERT: DOG detected with 97.2% confidence — dog_4859.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████████████████▏                                                        | 23/75 [00:03<00:11,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📣 ALERT: DOG detected with 99.1% confidence — dog_4860.jpeg\n",
      "📣 ALERT: DOG detected with 96.2% confidence — dog_4861.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████▎                                                      | 25/75 [00:04<00:12,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📣 ALERT: HUMAN detected with 20.1% confidence — dog_4862.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████████████████████▍                                                     | 26/75 [00:04<00:13,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📣 ALERT: DOG detected with 98.6% confidence — dog_4863.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████████████████▌                                                    | 27/75 [00:05<00:09,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📣 ALERT: ELEPHANT detected with 100.0% confidence — elephant_0002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[341]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m img_path = \u001b[33m\"\u001b[39m\u001b[33mTest_img/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m mask_path = \u001b[33m\"\u001b[39m\u001b[33mtest_mask/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m animal_detections = \u001b[43mrun_batch_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# For humans (change path accordingly if needed)\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# img_path = \"segmentation-full-body-mads-dataset/segmentation_full_body_mads_dataset_1192_img/segmentation_full_body_mads_dataset_1192_img/images\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# mask_path = \"segmentation-full-body-mads-dataset/segmentation_full_body_mads_dataset_1192_img/segmentation_full_body_mads_dataset_1192_img/masks\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# human_detections = run_batch_detection(img_path, mask_path)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[335]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mrun_batch_detection\u001b[39m\u001b[34m(image_root, mask_root)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     26\u001b[39m     cropped = extract_obj_from_mask(image_path, mask_path)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     label, prob = \u001b[43mclassify_with_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcropped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m📣 ALERT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m detected with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprob\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m confidence — \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m     all_detections.append((img_file, label, prob))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[257]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mclassify_with_clip\u001b[39m\u001b[34m(pil_image, labels, text_inputs)\u001b[39m\n\u001b[32m      2\u001b[39m image_input = preprocess(pil_image).unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     image_features = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     text_features = model.encode_text(text_inputs)\n\u001b[32m      6\u001b[39m     image_features /= image_features.norm(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\clip\\model.py:341\u001b[39m, in \u001b[36mCLIP.encode_image\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\clip\\model.py:232\u001b[39m, in \u001b[36mVisionTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    229\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_pre(x)\n\u001b[32m    231\u001b[39m x = x.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m x = x.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n\u001b[32m    235\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_post(x[:, \u001b[32m0\u001b[39m, :])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\clip\\model.py:203\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\clip\\model.py:191\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m    190\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.attention(\u001b[38;5;28mself\u001b[39m.ln_1(x))\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\assignment\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# For animals\n",
    "img_path = \"Test_img/\"\n",
    "mask_path = \"test_mask/\"\n",
    "\n",
    "animal_detections = run_batch_detection(img_path, mask_path)\n",
    "\n",
    "# For humans (change path accordingly if needed)\n",
    "\n",
    "# img_path = \"segmentation-full-body-mads-dataset/segmentation_full_body_mads_dataset_1192_img/segmentation_full_body_mads_dataset_1192_img/images\"\n",
    "# mask_path = \"segmentation-full-body-mads-dataset/segmentation_full_body_mads_dataset_1192_img/segmentation_full_body_mads_dataset_1192_img/masks\"\n",
    "\n",
    "# human_detections = run_batch_detection(img_path, mask_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Task",
   "language": "python",
   "name": "assignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
