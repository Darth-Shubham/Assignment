{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87a08ad-08b8-40ae-8297-96cc8e247b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b08f41d9-69eb-41f9-9e83-db8ca732ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_obj_from_mask(img_path, mask_path, output_size=(244, 244)):\n",
    "    image = cv2.imread(img_path)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if image is None or mask is None:\n",
    "        raise ValueError(f\"Mask: {mask_path} or image: {img_path} not found\")\n",
    "\n",
    "    _, thresh = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not contours:\n",
    "        raise ValueError(f\"No contour found in {mask_path}\")\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(np.concatenate(contours))\n",
    "    cropped = image[y:y+h, x:x+w]\n",
    "    cropped_resized = cv2.resize(cropped, output_size)\n",
    "    return Image.fromarray(cv2.cvtColor(cropped_resized, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e51403b-a5cb-4adc-92b6-aa04fa4379f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_center_crop(frame, size_ratio=0.5):\n",
    "    h, w = frame.shape[:2]\n",
    "    ch, cw = int(h * size_ratio), int(w * size_ratio)\n",
    "    if ch == 0 or cw == 0:\n",
    "        raise ValueError(\"Frame too small for cropping\")\n",
    "    x1, y1 = (w - cw) // 2, (h - ch) // 2\n",
    "    return frame[y1:y1 + ch, x1:x1 + cw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e15eed8-1905-4283-9f9b-7953a0a4a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_clip(pil_img, model, preprocess, text_inputs, labels, device):\n",
    "    img_input = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_features = model.encode_image(img_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "        img_features /= img_features.norm(dim=1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        similarity = (100.0 * img_features @ text_features.T).softmax(dim=-1)\n",
    "        top_prob, top_label_idx = similarity[0].max(0)\n",
    "\n",
    "    return labels[top_label_idx], top_prob.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6258e27d-82cc-4d45-a68f-6e0fb24dd5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_clip_model(labels):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {label}\") for label in labels]).to(device)\n",
    "    return model, preprocess, text_inputs, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216ab4ab-5abe-4cb7-9dfb-a29b9633622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_detection(image_root, mask_root, model, preprocess, text_inputs, labels, device):\n",
    "    all_detections = []\n",
    "\n",
    "    for class_folder in os.listdir(image_root):\n",
    "        image_folder = os.path.join(image_root, class_folder)\n",
    "        mask_folder = os.path.join(mask_root, class_folder)\n",
    "\n",
    "        if not os.path.isdir(image_folder):\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"üîç Scanning folder: {class_folder}\")\n",
    "        for img_file in tqdm(os.listdir(image_folder)):\n",
    "            if not img_file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "                continue\n",
    "\n",
    "            image_path = os.path.join(image_folder, img_file)\n",
    "            mask_path = os.path.join(mask_folder, os.path.splitext(img_file)[0] + \"_mask.png\")\n",
    "\n",
    "            try:\n",
    "                if os.path.exists(mask_path):\n",
    "                    cropped = extract_obj_from_mask(image_path, mask_path)\n",
    "                else:\n",
    "                    frame = cv2.imread(image_path)\n",
    "                    if frame is None:\n",
    "                        logging.warning(f\"‚ö†Ô∏è Skipping {img_file} ‚Äî image could not be loaded.\")\n",
    "                        continue\n",
    "                    cropped = extract_center_crop(frame)\n",
    "                    cropped = Image.fromarray(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                label, prob = classify_with_clip(cropped, model, preprocess, text_inputs, labels, device)\n",
    "                logging.info(f\"üì£ ALERT: {label.upper()} detected with {prob:.1%} confidence ‚Äî {img_file}\")\n",
    "                all_detections.append((img_file, label, prob))\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"‚ùå Error processing {img_file}: {e}\")\n",
    "\n",
    "    return all_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8484abc9-b67e-4b25-8b8d-8f80c7bb463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_video_detection(model, preprocess, text_inputs, labels, device, video_source=0):\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    logging.info(\"üé• Starting video stream...\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            crop = extract_center_crop(frame)\n",
    "            crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "            label, prob = classify_with_clip(crop_pil, model, preprocess, text_inputs, labels, device)\n",
    "            alert_text = f\"{label.upper()} ({prob*100:.1f}%)\"\n",
    "            cv2.putText(frame, alert_text, (30, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Frame processing error: {e}\")\n",
    "\n",
    "        cv2.imshow(\"Live Detection\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            logging.info(\"üëã Exiting...\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55e318e9-6318-4cef-9f8f-f02e6eb6717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_detections(detections, output_path=\"detections.csv\"):\n",
    "    with open(output_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Image\", \"Label\", \"Confidence\"])\n",
    "        writer.writerows(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75d9207e-a1ef-4e4d-aba7-3edcce548d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: üé• Starting video stream...\n",
      "INFO: üëã Exiting...\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    labels = [\"human\", \"butterfly\", \"dog\", \"cat\", \"horse\", \"elephant\", \"squirrel\"]\n",
    "    model, preprocess, text_inputs, device = setup_clip_model(labels)\n",
    "\n",
    "    # Optional test\n",
    "    # sample_img = \"animals10/raw-img/gatto/1014.jpeg\"\n",
    "    # sample_mask = \"animals10/renamed_masks/horse/horse_0001_mask.png\"\n",
    "    # try:\n",
    "    #     cropped = extract_obj_from_mask(sample_img, sample_mask)\n",
    "    #     cropped.show()\n",
    "    #     label, prob = classify_with_clip(cropped, model, preprocess, text_inputs, labels, device)\n",
    "    #     print(f\"üîî Detected: {label} (confidence: {prob:.2%})\")\n",
    "    # except Exception as e:\n",
    "    #     logging.error(f\"Sample test failed: {e}\")\n",
    "\n",
    "    # Batch mode\n",
    "    # detections = run_batch_detection(\"Test_img/\", \"test_mask/All\", model, preprocess, text_inputs, labels, device)\n",
    "    # save_detections(detections)\n",
    "\n",
    "    # Uncomment to run video detection\n",
    "    run_video_detection(model, preprocess, text_inputs, labels, device, \"Untitled design.mp4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Task",
   "language": "python",
   "name": "assignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
